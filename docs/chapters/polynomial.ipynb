{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "## Motivation\n",
    "\n",
    "```{tab} Economics & Finance\n",
    "* Modeling the relationship between advertising expenditure and sales.\n",
    "* Analyzing the relationship between GDP growth and inflation rates.\n",
    "* Predicting stock prices based on historical data.\n",
    "```\n",
    "\n",
    "```{tab} Medicine\n",
    "* Predicting patient outcomes based on various medical parameters.\n",
    "* Analyzing the relationship between dosage and drug effectiveness.\n",
    "```\n",
    "\n",
    "```{tab} Meteorology\n",
    "* Forecasting temperature trends based on historical data.\n",
    "* Predicting rainfall patterns using atmospheric variables.\n",
    "```\n",
    "\n",
    "```{tab} Environmental Science\n",
    "* Modeling the relationship between pollution levels and health outcomes.\n",
    "* Predicting carbon dioxide emissions based on industrial activity.\n",
    "```\n",
    "\n",
    "```{tab} Engineering\n",
    "* Predicting the strength of materials based on temperature and pressure.\n",
    "* Analyzing the relationship between fuel efficiency and vehicle speed.\n",
    "```\n",
    "\n",
    "```{tab} Psychology\n",
    "* Predicting academic performance based on study habits and socioeconomic factors.\n",
    "* Analyzing the relationship between job satisfaction and workplace conditions.\n",
    "```\n",
    "\n",
    "```{tab} Marketing\n",
    "* Predicting customer purchasing behavior based on demographic data.\n",
    "* Analyzing the relationship between pricing strategies and product demand.\n",
    "```\n",
    "\n",
    "```{tab} Agriculture\n",
    "* Predicting crop yields based on weather conditions and soil quality.\n",
    "* Analyzing the relationship between fertilizer usage and plant growth.\n",
    "```\n",
    "\n",
    "```{tab} Education\n",
    "* Predicting student performance based on factors like attendance and study time.\n",
    "* Analyzing the relationship between teacher qualifications and student outcomes.\n",
    "```\n",
    "\n",
    "```{tab} Sociology\n",
    "* Modeling the relationship between income inequality and crime rates.\n",
    "* Predicting population growth based on historical census data.\n",
    "```\n",
    "\n",
    "```{tab} Manufacturing\n",
    "* Predicting equipment failure rates based on operating conditions and maintenance schedules.\n",
    "* Analyzing the relationship between production volume and energy consumption.\n",
    "```\n",
    "\n",
    "```{tab} Public Health\n",
    "* Analyzing the relationship between vaccination rates and disease outbreaks.\n",
    "* Predicting the spread of infectious diseases based on population density and travel patterns.\n",
    "```\n",
    "\n",
    "```{tab} Transportation\n",
    "* Modeling traffic congestion based on time of day and road conditions.\n",
    "* Predicting vehicle accident rates based on factors like weather and road design.\n",
    "```\n",
    "\n",
    "\n",
    "## Regressing Data to a Polynomial Curve\n",
    "\n",
    "Suppose we have a set of $N$ data points $S = \\{ s_1, s_2, \\ldots, s_N \\}$ with each point $s_i = (x_i, y_i)$.\n",
    "\n",
    "We will regress the data to some degree $k$ polynomial $f(x)$ (see Equation {eq}`poly`) while minimizing the sum of the squares of the residual values SSR (see Equation {eq}`ssr`). {cite}`Krishnan_2018`\n",
    "\n",
    "```{admonition} What do we mean by \"regress\"?\n",
    ":class: note\n",
    "The term *regression* was first introduced by Sir Francis Galton. In his work he noted that the heights of children of both tall and short parents appeared to \"regress\" to toward the mean of the group. Note that regression analysis and the method of least squares are generally considered synonymous terms even though they are distinct concepts. {cite}`Regression`\n",
    "\n",
    "We will see that when we regress our data to the curve of best fit using the method of least squares, we will minimize the sum of the squares of the residuals (SSR).\n",
    "```\n",
    "\n",
    "$$\n",
    "\\require{physics}\n",
    "\n",
    "f(x) = \\underbrace{ \\alpha_0 + \\alpha_1 x + \\alpha_2 x^2 + \\ldots + \\alpha_k x^k }_{ \\text{$k + 1$ Terms} }\n",
    "$$ (poly)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\SSR(\\alpha_0, \\alpha_1, \\ldots, \\alpha_k)\n",
    "        &= \\sum_{i = 1}^N r_i^2 = \\sum_{i = 1}^N (y_i - \\hat{y_i})^2 = \\sum_{i = 1}^N (\\hat{y_i} - y_i)^2\\\\\n",
    "        &= \\sum_{i = 1}^N (f(x_i) - y_i)^2\\\\\n",
    "        &= \\sum_{i = 1}^N ( [\\alpha_0 + \\alpha_1 x_i + \\alpha_2 x_i^2 + \\ldots + \\alpha_k x_i^k] - y_i)^2\n",
    "\\end{align}\n",
    "$$ (ssr)\n",
    "\n",
    "To find some polynomial $\\alpha_0 + \\alpha_1 x + \\ldots + \\alpha_k x^k$ with minimal SSR value we should look for when $\\nabla \\text{SSR} = \\0$. In other words, we should see where the SSR value has an extrema (in our case, the only extrema will be a minima -- prove this yourself). So,\n",
    "\n",
    "$$\n",
    "\\nabla \\SSR(\\alpha_0, \\alpha_1, \\ldots, \\alpha_k)\n",
    "    = \\begin{bmatrix}\n",
    "        \\pdv{\\SSR}{\\alpha_0}\\\\\n",
    "        \\pdv{\\SSR}{\\alpha_1}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\pdv{\\SSR}{\\alpha_k}\\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\0\n",
    "$$ (grad)\n",
    "\n",
    "Now consider the partial derivative of SSR with respect to $\\alpha_j$ for some natural number $1 \\leq j \\leq k$. Then,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pdv{\\SSR}{\\alpha_j} &= \\pdv{\\alpha_j} \\left[ \\sum_{i = 1}^N (f(x) - y_i)^2 \\right]\\\\\n",
    "    &= \\sum_{i = 1}^N \\pdv{\\alpha_j} ( f(x) - y_i)^2\\\\\n",
    "    &= \\sum_{i = 1}^N 2 (f(x) - y_i)^1 \\cdot \\pdv{\\alpha_j}\\left[f(x) - y_i\\right]\\\\\n",
    "    &= 2 \\sum_{i = 1}^N (f(x) - y_i) \\cdot \\cancelto{ x_i ^ j }{ \\pdv{\\alpha_j}\\left[(\\alpha_0 + \\alpha_1 x_i + \\alpha_2 x_i^2 + \\ldots + \\alpha_k x_i^k) - y_i\\right] }\\\\\n",
    "    &= 2 \\sum_{i = 1}^N (f(x) - y_i) \\cdot x_i ^ j\\\\\n",
    "    &= 2 \\sum_{i = 1}^N [(\\alpha_0 + \\alpha_1 x_i + \\alpha_2 x_i^2 + \\ldots + \\alpha_k x_i^k) - y_i] \\cdot x_i ^ j\\\\\n",
    "    \\pdv{\\SSR}{\\alpha_j} &= 2 \\sum_{i = 1}^N \\left[ (\\alpha_0 x_i^j + \\alpha_1 x_i^{j + 1} + \\alpha_2 x_i^{j + 2} + \\ldots + \\alpha_k x_i^{j + k}) - y_i x_i^j \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can substitute this result into the vector in Equation {eq}`grad`.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "        \\pdv{\\SSR}{\\alpha_0}\\\\\n",
    "        \\pdv{\\SSR}{\\alpha_1}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\pdv{\\SSR}{\\alpha_k}\\\\\n",
    "    \\end{bmatrix}\n",
    "    &= \\0\\\\\n",
    "    \\begin{bmatrix}\n",
    "        \\ds 2 \\sum_{i = 1}^N \\left[ (\\alpha_0 x_i^0 + \\alpha_1 x_i^{1} + \\alpha_2 x_i^{2} + \\ldots + \\alpha_k x_i^{k}) - y_i x_i^0 \\right]\\\\\n",
    "        \\ds 2 \\sum_{i = 1}^N \\left[ (\\alpha_0 x_i^1 + \\alpha_1 x_i^{2} + \\alpha_2 x_i^{3} + \\ldots + \\alpha_k x_i^{k + 1}) - y_i x_i^1 \\right]\\\\\n",
    "        \\vdots\\\\\n",
    "        \\ds 2 \\sum_{i = 1}^N \\left[ (\\alpha_0 x_i^k + \\alpha_1 x_i^{k + 1} + \\alpha_2 x_i^{k + 2} + \\ldots + \\alpha_k x_i^{2k}) - y_i x_i^k \\right]\n",
    "    \\end{bmatrix}\n",
    "    &= \\0\\\\\n",
    "    \\cancel{2} \\sum_{i = 1}^N \\begin{bmatrix}\n",
    "        (\\alpha_0 x_i^0 + \\alpha_1 x_i^{1} + \\alpha_2 x_i^{2} + \\ldots + \\alpha_k x_i^{k}) - y_i x_i^0\\\\\n",
    "        (\\alpha_0 x_i^1 + \\alpha_1 x_i^{2} + \\alpha_2 x_i^{3} + \\ldots + \\alpha_k x_i^{k + 1}) - y_i x_i^1\\\\\n",
    "        \\vdots\\\\\n",
    "        (\\alpha_0 x_i^k + \\alpha_1 x_i^{k + 1} + \\alpha_2 x_i^{k + 2} + \\ldots + \\alpha_k x_i^{2k}) - y_i x_i^k\n",
    "    \\end{bmatrix}\n",
    "    &= \\0\\\\\n",
    "    \\sum_{i = 1}^N \\begin{bmatrix}\n",
    "        \\alpha_0 x_i^0 + \\alpha_1 x_i^{1} + \\alpha_2 x_i^{2} + \\ldots + \\alpha_k x_i^{k}\\\\\n",
    "        \\alpha_0 x_i^1 + \\alpha_1 x_i^{2} + \\alpha_2 x_i^{3} + \\ldots + \\alpha_k x_i^{k + 1}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\alpha_0 x_i^k + \\alpha_1 x_i^{k + 1} + \\alpha_2 x_i^{k + 2} + \\ldots + \\alpha_k x_i^{2k}\n",
    "    \\end{bmatrix}\n",
    "    - \\sum_{i = 1}^N \\begin{bmatrix}\n",
    "        y_i x_i^0\\\\\n",
    "        y_i x_i^1\\\\\n",
    "        \\vdots\\\\\n",
    "        y_i x_i^k\n",
    "    \\end{bmatrix}\n",
    "    &= \\0\\\\\n",
    "    \\color{orange}\n",
    "        \\sum_{i = 1}^N \\underbrace{\n",
    "        \\begin{bmatrix}\n",
    "            1 & x_i & x_i^{2} & \\cdots & x_i^{k}\\\\\n",
    "            x_i & x_i^{2} & x_i^{3} & \\cdots & x_i^{k + 1}\\\\\n",
    "            x_i^{2} & x_i^{3} & x_i^{4} & \\cdots & x_i^{k + 1}\\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "            x_i^k & x_i^{k + 1} & x_i^{k + 2} & \\ldots & x_i^{2k}\n",
    "        \\end{bmatrix}\n",
    "    }_{ (k + 1) \\times (k + 1) }\n",
    "        \\color{red}\n",
    "        \\begin{bmatrix}\n",
    "            \\alpha_0\\\\\n",
    "            \\alpha_1\\\\\n",
    "            \\alpha_2\\\\\n",
    "            \\vdots\\\\\n",
    "            \\alpha_k\n",
    "        \\end{bmatrix}\n",
    "        \\color{black}\n",
    "    &= \\color{blue}\n",
    "        \\sum_{i = 1}^N \\left.\n",
    "        \\begin{bmatrix}\n",
    "            y_i\\\\\n",
    "            y_i x_i\\\\\n",
    "            y_i x_i^2\\\\\n",
    "            \\vdots\\\\\n",
    "            y_i x_i^k\n",
    "        \\end{bmatrix}\n",
    "    \\right\\} (k + 1) \\times 1\\\\\n",
    "    \\color{orange} M \\color{red} \\vec{\\alpha} \\color{black} &= \n",
    "    \\color{blue} \\vec{\\beta}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regressing Data Using Numpy\n",
    "\n",
    "Now that we have a general equation that will allow us to regress some data $S$ to a polynomial of degree $k$, we can write some code that will solve the equation for us.\n",
    "\n",
    "We will do this using `numpy`, a Python package that allows us to work with vectors, matrices, and use many of the methods we employ in Linear Algebra out-of-the-box.\n",
    "\n",
    "First, let's define a function called `poly_regress` that will take take our data `S` and the degree of the polynomial to regress to `k` and return the coefficients of the regressed polynomial `alpha` (the coefficients will be ordered as $\\alpha_0, \\alpha_1, \\ldots, \\alpha_k$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a7a8a18419c2057"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a340465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def poly_regress(S: np.ndarray, k: int) -> np.ndarray:\n",
    "    M = np.zeros((k + 1, k + 1))\n",
    "    b = np.zeros(k + 1)\n",
    "    for xi, yi in S:\n",
    "        powers_of_xi = np.power(xi, np.arange(k + 1))\n",
    "        M_i = np.outer(powers_of_xi, powers_of_xi)\n",
    "\n",
    "        M += M_i\n",
    "        b += yi * powers_of_xi\n",
    "\n",
    "    alpha = np.linalg.solve(M, b)  # Note: Mx = b may not have a *unique* solution (this returns one of them)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's not try out our function using some data.\n",
    "\n",
    "The function `poly_to_str` is just a \"helper\" function that prints the polynomial corresponding to $\\alpha = [\\alpha_0, \\alpha_1, \\ldots, \\alpha_k]$ as $\\alpha_k x^k + \\alpha_{k - 1} x^{k - 1} + \\ldots + \\alpha_0 x^0$ (notice that the order of the coefficients is reversed)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bd6d23b956ab05b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe659442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from array_to_latex import to_ltx\n",
    "from IPython.core.display import Math\n",
    "from IPython.display import display\n",
    "\n",
    "k = 2  # degree of the polynomial to regress against\n",
    "S = np.array([[-2, -4], [0, 2], [2, +3], [5, 6], [-5, 5]])  # data points\n",
    "\n",
    "alpha = poly_regress(S, k)  # regress the data against a polynomial of degree k (we get the polynomial's coefficients)\n",
    "\n",
    "\n",
    "# Helper function to print a polynomial as a_k x^k \n",
    "def poly_to_str(alpha: np.ndarray) -> str:\n",
    "    n_coeffs = len(alpha)\n",
    "    return \" + \".join([f\"{a_i:.3f} x^{n_coeffs - i - 1}\" for i, a_i in enumerate(reversed(alpha))])\n",
    "\n",
    "\n",
    "def poly_to_math(alpha: np.ndarray, prefix='') -> Math:\n",
    "    n_coeffs = len(alpha)\n",
    "    poly_latex = \" + \".join([f\"{a_i:.3f} x^{n_coeffs - i - 1}\" for i, a_i in enumerate(reversed(alpha))])\n",
    "    return Math(f'{prefix}{poly_latex}')\n",
    "\n",
    "\n",
    "# (i) Print as text\n",
    "# Note: the coefficients in alpha and in f(x) are reversed (this is an arbitrary decision)\n",
    "print(f'alpha = {np.array2string(alpha, precision=3)}')\n",
    "print(f'f(x) = {poly_to_str(alpha)}')  # print the equation of the polynomial\n",
    "\n",
    "# (ii) Display as Math\n",
    "latex_formatter = get_ipython().display_formatter.formatters[\"text/latex\"]\n",
    "latex_formatter.for_type(np.ndarray, lambda m: to_ltx(m, print_out=False))\n",
    "\n",
    "display(alpha, poly_to_math(alpha, prefix='f(x) = '))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def construct_poly_curve(alpha: np.ndarray, x_lims=(-5, +5), step_size=0.5, **kwargs):\n",
    "    min_x, max_x = x_lims\n",
    "    delta = (max_x - min_x)\n",
    "    n_steps = int(np.ceil(delta / step_size))\n",
    "\n",
    "    x = np.linspace(min_x, max_x, n_steps)\n",
    "    powers_of_x = np.power(x[:, np.newaxis], np.arange(len(alpha)))\n",
    "    y = np.dot(powers_of_x, alpha)\n",
    "\n",
    "    return go.Scatter(x=x, y=y, mode='lines', **kwargs)\n",
    "\n",
    "\n",
    "def construct_regression_poly_curve(data: np.ndarray, degree: int, **kwargs):\n",
    "    return construct_poly_curve(poly_regress(data, degree), **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5463fcaef37152c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from plotly.graph_objs.layout import XAxis, YAxis, Title\n",
    "\n",
    "x, y = S.T\n",
    "\n",
    "data_plot = go.Scatter(x=x, y=y, mode='markers', name='Data')\n",
    "poly_curve = construct_poly_curve(alpha, name=f'Regression', x_lims=(-100, +100))\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=Title(text=f'Polynomial Regressions (k = {k})', x=0.5),\n",
    "    xaxis=XAxis(title='X', range=[-10, 10]),\n",
    "    yaxis=YAxis(title='Y', range=[-20, 20], scaleanchor='x', scaleratio=1)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[data_plot, poly_curve], layout=layout)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3beb79c830dbc943",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from plotly.graph_objs.layout import XAxis, YAxis, Title\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "k_max = 10  # We will regress for k = 0, 1, ..., 9\n",
    "regressions = [construct_regression_poly_curve(S, k,\n",
    "                                               name=f'k = {k}',\n",
    "                                               visible='legendonly',\n",
    "                                               x_lims=(-100, +100))\n",
    "               for k in range(k_max)]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=Title(text='Polynomial Regressions', x=0.5),\n",
    "    xaxis=XAxis(title='X-axis', range=[-10, 10]),\n",
    "    yaxis=YAxis(title='Y-axis', range=[-20, 20], scaleanchor='x', scaleratio=1)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[data_plot, *regressions], layout=layout)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece1875d499b6584",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{bibliography}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9794c091e5169b3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
